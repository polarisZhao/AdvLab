# :star2:adversarial attack and defense :star2:

## :books: Papers

#### 1. General & Survey

- [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572), I. Goodfellow et al., ICLR 2015
- [Threat of Adversarial Attacks on Deep Learning in Computer Vision - A Survey](https://arxiv.org/abs/1801.00553), Akhtar N, Mian A. 

#### 2. Attack

- **FGSM: **[Adversarial examples in the physical world](https://arxiv.org/abs/1607.02533) Kurakin, Alexey, Ian Goodfellow, and Samy Bengio. arXiv preprint arXiv:1607.02533 (2016).
- **JSMA: **[The limitations of deep learning in adversarial settings](http://ieeexplore.ieee.org/abstract/document/7467366/) Papernot, Nicolas, et al. Security and Privacy (EuroS&P), 2016 IEEE European Symposium on. IEEE, 2016.
- **DeepFool: ** [Deepfool: a simple and accurate method to fool deep neural networks](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Moosavi-Dezfooli_DeepFool_A_Simple_CVPR_2016_paper.html) Moosavi-Dezfooli, Seyed-Mohsen, Alhussein Fawzi, and Pascal Frossard. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.
- **C&W: **[Towards evaluating the robustness of neural networks](https://arxiv.org/abs/1608.04644) Carlini, Nicholas, and David Wagner. Security and Privacy (S&P). 2017.
- **One Pixel attack: **[One pixel attack for fooling deep neural networks](https://arxiv.org/abs/1710.08864)

#### 3. Defense

##### network Ditillation

##### adversarial  Training

##### adversarial Detecting

##### Classifier Robustifying

#### 4. Others

- [Adversarial Attacks on Neural Networks for Graph Data](https://arxiv.org/abs/1805.07984v3)
- [Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples](https://arxiv.org/pdf/1802.00420)



## Conference and Workshop

- [Artificial Intelligence for Cyber Security (AICS)](http://www-personal.umich.edu/~arunesh/AICS2019/index.html)
- [NeuralIPS workshop 2018 Workshop on Security in Machine Learning](https://secml2018.github.io/)
- [1ST DEEP LEARNING AND SECURITY WORKSHOP](https://www.ieee-security.org/TC/SPW2018/DLS/)
- [39th IEEE Symposium on Security and Privacy Workshops](https://www.ieee-security.org/TC/SP2018/workshops.html)
- [1st Workshop on Recent Advances in Adversarial Machine Learning](https://www.research.ibm.com/labs/ireland/nemesis2018/)



## Library

- **cleverhans by Google **  [**github**](https://github.com/tensorflow/cleverhans)
- **AdvBox by Baidu**   [**github**](https://github.com/baidu/AdvBox)
- **adversarial-robustness-toolbox by IBM**  [**github**](https://github.com/IBM/adversarial-robustness-toolbox)
- **foolbox**   [**github**](https://github.com/bethgelab/foolbox)

## Competition & TopSolution

- Kaggle 

  [NIPS 2017: Defense Against Adversarial Attack](https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack)

  [NIPS 2017: Targeted Adversarial Attack](https://www.kaggle.com/c/nips-2017-targeted-adversarial-attack)

  [NIPS 2017: Non-targeted Adversarial Attack](https://www.kaggle.com/c/nips-2017-non-targeted-adversarial-attack)

- Crowdai

  [NIPS 2018 : Adversarial Vision Challenge (Targeted Attack Track)](https://www.crowdai.org/challenges/nips-2018-adversarial-vision-challenge-targeted-attack-track)

  [NIPS 2018 : Adversarial Vision Challenge (Robust Model Track)](https://www.crowdai.org/challenges/nips-2018-adversarial-vision-challenge-robust-model-track)

  [NIPS 2018 : Adversarial Vision Challenge (Untargeted Attack Track)](https://www.crowdai.org/challenges/nips-2018-adversarial-vision-challenge-untargeted-attack-track)

- GeekPwn 2018 -- [CAAD](https://en.caad.geekpwn.org)

  TOP1 solution
  Blog: http://ppwwyyxx.com/2018/Geekpwn-CAAD-CTF/

  GitHub: https://github.com/ppwwyyxx/Adversarial-Face-Attack

- Codalab

  [MCS 2018. Adversarial Attacks on Black Box Face Recognition](https://competitions.codalab.org/competitions/19090#learn_the_details)

  TOP 2 solution 
  github : https://github.com/snakers4/msc-2018-final

## Talks

- [Do Statistical Models Understand the World?](https://www.youtube.com/watch?v=Pq4A2mPCB0Y), I. Goodfellow, 2015
- [Classifiers under Attack](https://www.usenix.org/conference/enigma2017/conference-program/presentation/evans), David Evans, 2017
- [Adversarial Examples in Machine Learning](https://www.usenix.org/conference/enigma2017/conference-program/presentation/papernot), Nicolas Papernot, 2017
- [Poisoning Behavioral Malware Clustering](http://pralab.diee.unica.it/en/node/1121), Biggio. B, Rieck. K, Ariu. D, Wressnegger. C, Corona. I. Giacinto, G. Roli. F, 2014
- [Is Data Clustering in Adversarial Settings Secure?](http://pralab.diee.unica.it/en/node/955), BBiggio. B, Pillai. I, Rota Bulò. S, Ariu. D, Pelillo. M, Roli. F, 2015
- [Poisoning complete-linkage hierarchical clustering](http://pralab.diee.unica.it/en/node/1089), Biggio. B, Rota Bulò. S, Pillai. I, Mura. M, Zemene Mequanint. E, Pelillo. M, Roli. F, 2014
- [Is Feature Selection Secure against Training Data Poisoning?](https://pralab.diee.unica.it/en/node/1191), Xiao. H, Biggio. B, Brown. G, Fumera. G, Eckert. C, Roli. F, 2015
- [Adversarial Feature Selection Against Evasion Attacks](https://pralab.diee.unica.it/en/node/1188), 	Zhang. F, Chan. PPK, Biggio. B, Yeung. DS, Roli. F, 2016

## Blogs

- [Breaking Linear Classifiers on ImageNet](http://karpathy.github.io/2015/03/30/breaking-convnets/), A. Karpathy et al.
- [Breaking things is easy](http://www.cleverhans.io/security/privacy/ml/2016/12/16/breaking-things-is-easy.html), N. Papernot & I. Goodfellow et al.
- [Attacking Machine Learning with Adversarial Examples](https://blog.openai.com/adversarial-example-research/), N. Papernot, I. Goodfellow, S. Huang, Y. Duan, P. Abbeel, J. Clark.
- [Robust Adversarial Examples](https://blog.openai.com/robust-adversarial-inputs/), Anish Athalye.
- [A Brief Introduction to Adversarial Examples](http://people.csail.mit.edu/madry/lab/blog/adversarial/2018/07/06/adversarial_intro/), A. Madry et al.
- [Training Robust Classifiers (Part 1)](http://people.csail.mit.edu/madry/lab/blog/adversarial/2018/07/11/robust_optimization_part1/), A. Madry et al.
- [Adversarial Machine Learning Reading List](https://nicholas.carlini.com/writing/2018/adversarial-machine-learning-reading-list.html), N. Carlini
- [Recommendations for Evaluating Adversarial Example Defenses](https://nicholas.carlini.com/writing/2018/evaluating-adversarial-example-defenses.html), N. Carlini
- [Security of Machine Learning](https://www.pluribus-one.it/sec-ml/sec-ml-research-blog) Pluribus-one

## Lab

- [MADRY LAB](https://people.csail.mit.edu/madry/lab/)



